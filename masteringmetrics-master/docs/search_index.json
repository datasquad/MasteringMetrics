[
["index.html", "R Code for Mastering ’Metrics Welcome Install License Colonophon", " R Code for Mastering ’Metrics Jeffrey B. Arnold Welcome This work contains R code to reproduce many of the analyses in Mastering ’Metrics by Joshua D. Angrist and Jörn-Steffen Pischke (Angrist and Pischke 2014). This work provides R translations of the replication code available at masteringmetrics.com. The R code used in the examples heavily depends on tidyverse packages. I suggest starting with Grolemund and Wickham, R for Data Science if you are unfamiliar with the tidyverse. Install To install all R packages and datasets needed to run the examples in Mastering ’Metrics run: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;jrnold/masteringmetrics&quot;, subdir = &quot;masteringmetrics&quot;) License The text of this work is licensed under the Creative Commons Attribution 4.0 International License. The R Code in this work is licensed under the MIT License. Colonophon The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: devtools::session_info(c(&quot;tidyverse&quot;)) #&gt; Session info ------------------------------------------------------------- #&gt; setting value #&gt; version R version 3.4.4 (2018-03-15) #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; tz America/Los_Angeles #&gt; date 2018-04-15 #&gt; Packages ----------------------------------------------------------------- #&gt; package * version date source #&gt; assertthat 0.2.0 2017-04-11 CRAN (R 3.4.0) #&gt; backports 1.1.2 2017-12-13 CRAN (R 3.4.3) #&gt; base64enc 0.1-3 2015-07-28 CRAN (R 3.4.0) #&gt; BH 1.66.0-1 2018-02-13 CRAN (R 3.4.3) #&gt; bindr 0.1.1 2018-03-13 CRAN (R 3.4.4) #&gt; bindrcpp 0.2 2017-06-17 CRAN (R 3.4.0) #&gt; broom 0.4.4 2018-03-29 cran (@0.4.4) #&gt; callr 2.0.2 2018-02-11 CRAN (R 3.4.3) #&gt; cellranger 1.1.0 2016-07-27 CRAN (R 3.4.0) #&gt; cli 1.0.0 2017-11-05 cran (@1.0.0) #&gt; colorspace 1.3-2 2016-12-14 CRAN (R 3.4.0) #&gt; compiler 3.4.4 2018-03-15 local #&gt; crayon 1.3.4 2017-09-16 CRAN (R 3.4.1) #&gt; curl 3.1 2017-12-12 CRAN (R 3.4.3) #&gt; DBI 0.8 2018-03-02 CRAN (R 3.4.3) #&gt; dbplyr 1.2.1 2018-02-19 CRAN (R 3.4.3) #&gt; debugme 1.1.0 2017-10-22 CRAN (R 3.4.2) #&gt; dichromat 2.0-0 2013-01-24 CRAN (R 3.4.0) #&gt; digest 0.6.15 2018-01-28 CRAN (R 3.4.3) #&gt; dplyr 0.7.4 2017-09-28 CRAN (R 3.4.2) #&gt; evaluate 0.10.1 2017-06-24 CRAN (R 3.4.1) #&gt; forcats 0.3.0 2018-02-19 CRAN (R 3.4.3) #&gt; foreign 0.8-69 2017-06-22 CRAN (R 3.4.4) #&gt; ggplot2 2.2.1 2016-12-30 CRAN (R 3.4.0) #&gt; glue 1.2.0 2017-10-29 CRAN (R 3.4.2) #&gt; graphics * 3.4.4 2018-03-15 local #&gt; grDevices * 3.4.4 2018-03-15 local #&gt; grid 3.4.4 2018-03-15 local #&gt; gtable 0.2.0 2016-02-26 CRAN (R 3.4.0) #&gt; haven 1.1.1.9000 2018-03-31 Github (tidyverse/haven@746eb3e) #&gt; highr 0.6 2016-05-09 CRAN (R 3.4.0) #&gt; hms 0.4.2 2018-03-10 CRAN (R 3.4.4) #&gt; htmltools 0.3.6 2017-04-28 CRAN (R 3.4.0) #&gt; httr 1.3.1 2017-08-20 CRAN (R 3.4.1) #&gt; jsonlite 1.5 2017-06-01 CRAN (R 3.4.0) #&gt; knitr 1.20 2018-02-20 CRAN (R 3.4.3) #&gt; labeling 0.3 2014-08-23 CRAN (R 3.4.0) #&gt; lattice 0.20-35 2017-03-25 CRAN (R 3.4.4) #&gt; lazyeval 0.2.1 2017-10-29 CRAN (R 3.4.2) #&gt; lubridate 1.7.3 2018-02-27 CRAN (R 3.4.3) #&gt; magrittr 1.5 2014-11-22 CRAN (R 3.4.0) #&gt; markdown 0.8 2017-04-20 CRAN (R 3.4.0) #&gt; MASS 7.3-49 2018-02-23 CRAN (R 3.4.3) #&gt; methods 3.4.4 2018-03-15 local #&gt; mime 0.5 2016-07-07 CRAN (R 3.4.0) #&gt; mnormt 1.5-5 2016-10-15 CRAN (R 3.4.0) #&gt; modelr 0.1.1 2017-07-24 CRAN (R 3.4.1) #&gt; munsell 0.4.3 2016-02-13 CRAN (R 3.4.0) #&gt; nlme 3.1-131.1 2018-02-16 CRAN (R 3.4.3) #&gt; openssl 1.0.1 2018-03-03 CRAN (R 3.4.3) #&gt; parallel 3.4.4 2018-03-15 local #&gt; pillar 1.2.1 2018-02-27 CRAN (R 3.4.3) #&gt; pkgconfig 2.0.1 2017-03-21 CRAN (R 3.4.0) #&gt; plogr 0.2.0 2018-03-25 CRAN (R 3.4.4) #&gt; plyr 1.8.4 2016-06-08 CRAN (R 3.4.0) #&gt; praise 1.0.0 2015-08-11 CRAN (R 3.4.0) #&gt; psych 1.7.8 2017-09-09 CRAN (R 3.4.1) #&gt; purrr 0.2.4 2017-10-18 cran (@0.2.4) #&gt; R6 2.2.2 2017-06-17 CRAN (R 3.4.0) #&gt; RColorBrewer 1.1-2 2014-12-07 CRAN (R 3.4.0) #&gt; Rcpp 0.12.16 2018-03-13 cran (@0.12.16) #&gt; readr 1.1.1 2017-05-16 CRAN (R 3.4.0) #&gt; readxl 1.0.0 2017-04-18 CRAN (R 3.4.0) #&gt; rematch 1.0.1 2016-04-21 CRAN (R 3.4.0) #&gt; reprex 0.1.2 2018-01-26 CRAN (R 3.4.3) #&gt; reshape2 1.4.3 2017-12-11 CRAN (R 3.4.3) #&gt; rlang 0.2.0 2018-02-20 CRAN (R 3.4.3) #&gt; rmarkdown 1.9 2018-03-01 CRAN (R 3.4.3) #&gt; rprojroot 1.3-2 2018-01-03 CRAN (R 3.4.3) #&gt; rstudioapi 0.7 2017-09-07 CRAN (R 3.4.1) #&gt; rvest 0.3.2 2016-06-17 CRAN (R 3.4.0) #&gt; scales 0.5.0 2017-08-24 CRAN (R 3.4.1) #&gt; selectr 0.3-2 2018-03-05 CRAN (R 3.4.4) #&gt; stats * 3.4.4 2018-03-15 local #&gt; stringi 1.1.7 2018-03-12 CRAN (R 3.4.4) #&gt; stringr 1.3.0 2018-02-19 CRAN (R 3.4.3) #&gt; testthat 2.0.0 2017-12-13 CRAN (R 3.4.3) #&gt; tibble 1.4.2 2018-01-22 CRAN (R 3.4.3) #&gt; tidyr 0.8.0 2018-01-29 CRAN (R 3.4.3) #&gt; tidyselect 0.2.4 2018-02-26 CRAN (R 3.4.3) #&gt; tidyverse 1.2.1 2017-11-14 CRAN (R 3.4.2) #&gt; tools 3.4.4 2018-03-15 local #&gt; utf8 1.1.3 2018-01-03 CRAN (R 3.4.3) #&gt; utils * 3.4.4 2018-03-15 local #&gt; viridisLite 0.3.0 2018-02-01 CRAN (R 3.4.3) #&gt; whisker 0.3-2 2013-04-28 CRAN (R 3.4.0) #&gt; withr 2.1.2 2018-03-15 CRAN (R 3.4.4) #&gt; xml2 1.2.0 2018-01-24 CRAN (R 3.4.3) #&gt; yaml 2.1.18 2018-03-08 cran (@2.1.18) "],
["national-health-interview-survey.html", "1 National Health Interview Survey 1.1 References", " 1 National Health Interview Survey This reproduces the analyses in Table 1.1 of Angrist and Pischke (2014). which compares people with and without health insurance in the 2009 National Health Interview Survey (NHIS). The code is derived from NHIS2009_hicompare.do. Load the prerequisite packages. library(&quot;tidyverse&quot;) library(&quot;magrittr&quot;) library(&quot;haven&quot;) Load the data (originally from http://masteringmetrics.com/wp-content/uploads/2015/01/Data.zip), and adjust a few of the columns to account for differences in how Stata and R store data. data(&quot;NHIS2009&quot;, package = &quot;masteringmetrics&quot;) Remove missing values. NHIS2009 &lt;- NHIS2009 %&gt;% filter(marradult, perweight != 0) %&gt;% group_by(serial) %&gt;% mutate(hi_hsb = mean(hi_hsb1, na.rm = TRUE)) %&gt;% filter(!is.na(hi_hsb), !is.na(hi)) %&gt;% mutate(female = sum(fml)) %&gt;% filter(female == 1) %&gt;% select(-female) For the sample only include married adults between 26 and 59 in age, and remove single person households. NHIS2009 &lt;- NHIS2009 %&gt;% filter(between(age, 26, 59), marradult, adltempl &gt;= 1) Keep only single family households. NHIS2009 &lt;- NHIS2009 %&gt;% group_by(serial) %&gt;% filter(length(serial) &gt; 1L) %&gt;% ungroup() Tables of wives and husbands by health insurance. status. The weighting following the “analytic” weights in the original .do file which weights observations by perweight and normalizes the weights so that the sub-samples of males and females have the same number as the original sample. NHIS2009 %&gt;% group_by(fml) %&gt;% # normalize person weights to match number of observations in each # group mutate(perweight = perweight / sum(perweight) * n()) %&gt;% group_by(fml, hi) %&gt;% summarise(n_wt = sum(perweight)) %&gt;% group_by(fml) %&gt;% mutate(prop = n_wt / sum(n_wt)) #&gt; # A tibble: 4 x 4 #&gt; # Groups: fml [2] #&gt; fml hi n_wt prop #&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 FALSE 0. 1281. 0.136 #&gt; 2 FALSE 1. 8114. 0.864 #&gt; 3 TRUE 0. 1131. 0.120 #&gt; 4 TRUE 1. 8264. 0.880 Compare sample statistics of mean and women, with and without health insurance. varlist &lt;- c(&quot;hlth&quot;, &quot;nwhite&quot;, &quot;age&quot;, &quot;yedu&quot;, &quot;famsize&quot;, &quot;empl&quot;, &quot;inc&quot;) NHIS2009_diff &lt;- NHIS2009 %&gt;% # rlang::set_attrs with NULL removes attributes from columns. # this avoids a warning from gather about differing attributes map_dfc(~ rlang::set_attrs(.x, NULL)) %&gt;% select(fml, hi, one_of(varlist)) %&gt;% gather(variable, value, -fml, -hi) %&gt;% group_by(fml, hi, variable) %&gt;% summarise(mean = mean(value, na.rm = TRUE), sd = sd(value, na.rm = TRUE)) %&gt;% gather(stat, value, -fml, -hi, -variable) %&gt;% unite(stat_hi, stat, hi) %&gt;% spread(stat_hi, value) %&gt;% mutate(diff = mean_1 - mean_0) knitr::kable(NHIS2009_diff, digits = 3) fml variable mean_0 mean_1 sd_0 sd_1 diff FALSE age 4.13e+01 4.42e+01 8.40e+00 8.61e+00 2.893 FALSE empl 8.52e-01 9.22e-01 3.55e-01 2.68e-01 0.070 FALSE famsize 4.06e+00 3.55e+00 1.54e+00 1.32e+00 -0.506 FALSE hlth 3.70e+00 3.98e+00 1.01e+00 9.34e-01 0.278 FALSE inc 4.36e+04 1.04e+05 3.57e+04 5.48e+04 60366.415 FALSE nwhite 1.88e-01 2.00e-01 3.91e-01 4.00e-01 0.011 FALSE yedu 1.12e+01 1.41e+01 3.47e+00 2.68e+00 2.919 TRUE age 3.95e+01 4.22e+01 8.26e+00 8.65e+00 2.631 TRUE empl 5.41e-01 7.58e-01 4.98e-01 4.29e-01 0.216 TRUE famsize 4.07e+00 3.55e+00 1.54e+00 1.32e+00 -0.520 TRUE hlth 3.61e+00 3.99e+00 1.02e+00 9.28e-01 0.382 TRUE inc 4.36e+04 1.03e+05 3.52e+04 5.51e+04 59722.242 TRUE nwhite 1.83e-01 2.02e-01 3.87e-01 4.01e-01 0.018 TRUE yedu 1.14e+01 1.43e+01 3.50e+00 2.60e+00 2.913 1.1 References http://masteringmetrics.com/wp-content/uploads/2014/12/ReadMe_NHIS.txt http://masteringmetrics.com/wp-content/uploads/2015/01/NHIS2009_hicompare.do "],
["rand-health-insurance-experiment-hie.html", "2 RAND Health Insurance Experiment (HIE) 2.1 Table 1.3 2.2 Table 1.4 References", " 2 RAND Health Insurance Experiment (HIE) This provides code replicates the Tables 1.3 and 1.4 of Angrist and Pischke (2014) which replicate the analyses from the RAND Health Insurance Experiment (Brook et al. 1983,@Aron-DineEinavEtAl2013). Load necessary libraries. library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;haven&quot;) library(&quot;rlang&quot;) library(&quot;clubSandwich&quot;) Function to calculate clustered standard errors and return a tidy data frame of the coefficients and standard errors. cluster_se &lt;- function(mod, cluster, type = &quot;CR2&quot;) { vcov &lt;- vcovCR(mod, cluster = cluster, type = type) coef_test(mod, vcov = vcov) %&gt;% rownames_to_column(var = &quot;term&quot;) %&gt;% as_tibble() %&gt;% select(term, estimate = beta, std.error = SE) } 2.1 Table 1.3 Angrist and Pischke (2014) Table 1.3 presents demographic and baseline health characteristics for subjects of the RAND Health Insurance Experiment (HIE). Load the rand data. data(&quot;rand_sample&quot;, package = &quot;masteringmetrics&quot;) Calculate the number in each plan: plantypes &lt;- count(rand_sample, plantype) knitr::kable(plantypes) plantype n Catastrophic 759 Deductible 881 Coinsurance 1022 Free 1295 For each variable variables, estimate the the difference in means between heath insurance plan types. varlist &lt;- c(&quot;female&quot;, &quot;blackhisp&quot;, &quot;age&quot;, &quot;educper&quot;, &quot;income1cpi&quot;, &quot;hosp&quot;, &quot;ghindx&quot;, &quot;cholest&quot;, &quot;diastol&quot;, &quot;systol&quot;, &quot;mhi&quot;, &quot;ghindxx&quot;, &quot;cholestx&quot;, &quot;diastolx&quot;, &quot;systolx&quot;, &quot;mhix&quot;) Create column (1) with the mean and standard deviation of the “Catastrophic” plan, catastrophic_stats &lt;- rand_sample %&gt;% filter(plantype == &quot;Catastrophic&quot;) %&gt;% select(one_of(varlist)) %&gt;% gather(variable, value) %&gt;% group_by(variable) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), `Std. Dev.` = sd(value, na.rm = TRUE)) knitr::kable(catastrophic_stats, digits = 3) variable Mean Std. Dev. age 3.24e+01 1.29e+01 blackhisp 1.72e-01 3.77e-01 cholest 2.07e+02 3.99e+01 cholestx 2.03e+02 4.21e+01 diastol 7.48e+01 1.10e+01 diastolx 7.88e+01 1.20e+01 educper 1.21e+01 2.88e+00 female 5.60e-01 4.97e-01 ghindx 7.09e+01 1.49e+01 ghindxx 6.85e+01 1.59e+01 hosp 1.15e-01 3.20e-01 income1cpi 3.16e+04 1.81e+04 mhi 7.38e+01 1.43e+01 mhix 7.55e+01 1.48e+01 systol 1.22e+02 1.65e+01 systolx 1.22e+02 1.87e+01 The difference in means between plans and the catastophic plan. calc_diffs &lt;- function(x) { # programmatically create the formula for lm f &lt;- quo(!!sym(x) ~ plantype) mod &lt;- lm(f, data = rand_sample) # nolint out &lt;- cluster_se(mod, cluster = rand_sample[[&quot;fam_identifier&quot;]]) out[[&quot;response&quot;]] &lt;- x out } plantype_diffs &lt;- map_dfr(varlist, calc_diffs) %&gt;% select(response, term, estimate, std.error) %&gt;% mutate(term = str_replace(term, &quot;^plantype&quot;, &quot;&quot;)) Create a table similar to Angrist and Pischke (2014) Table 1.3. fmt_num &lt;- function(x) { prettyNum(x, digits = 3, format = &quot;f&quot;, big.mark = &quot;,&quot;, drop0trailing = FALSE) } plantype_diffs %&gt;% mutate(estimate = str_c(fmt_num(estimate), &quot; (&quot;, fmt_num(std.error), &quot;)&quot;)) %&gt;% select(-std.error) %&gt;% spread(term, estimate) %&gt;% knitr::kable(digits = 3) response (Intercept) Coinsurance Deductible Free age 32.4 (0.485) 0.966 (0.655) 0.561 (0.676) 0.435 (0.614) blackhisp 0.172 (0.0199) -0.0269 (0.025) -0.0188 (0.0266) -0.0281 (0.0245) cholest 207 (1.99) -1.93 (2.76) -1.42 (2.99) -5.25 (2.7) cholestx 203 (1.87) -2.31 (2.47) 0.691 (2.58) -1.83 (2.39) diastol 74.8 (0.569) -0.514 (0.786) 1.22 (0.831) -0.143 (0.721) diastolx 78.8 (0.466) -0.335 (0.617) 0.219 (0.648) -1.03 (0.588) educper 12.1 (0.14) -0.0613 (0.186) -0.157 (0.191) -0.263 (0.183) female 0.56 (0.0118) -0.0247 (0.0153) -0.0231 (0.016) -0.0379 (0.015) ghindx 70.9 (0.694) 0.211 (0.922) -1.44 (0.952) -1.31 (0.872) ghindxx 68.5 (0.702) 0.612 (0.903) -0.869 (0.964) -0.776 (0.867) hosp 0.115 (0.0117) -0.00249 (0.0152) 0.00449 (0.016) 0.00117 (0.0146) income1cpi 31,603 (1,073) 970 (1,391) -2,104 (1,386) -976 (1,346) mhi 73.8 (0.619) 1.19 (0.81) -0.12 (0.822) 0.89 (0.766) mhix 75.5 (0.696) 1.07 (0.872) 0.454 (0.911) 0.433 (0.826) systol 122 (0.805) 0.907 (1.08) 2.32 (1.16) 1.12 (1.01) systolx 122 (0.782) -1.39 (0.986) 1.17 (1.06) -0.522 (0.934) Plot the difference-in-means of each plantype vs. catastrophic insurance. ggplot(filter(plantype_diffs, term != &quot;(Intercept)&quot;), aes(x = term, y = estimate, ymin = estimate - 2 * std.error, ymax = estimate + 2 * std.error)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 1) + geom_pointrange() + facet_grid(response ~ ., scales = &quot;free_y&quot;) 2.2 Table 1.4 Replicate Angrist and Pischke (2014) Table 1.4 which presents health outcome and health expenditure results from the RAND HIE. data(&quot;rand_person_spend&quot;, package = &quot;masteringmetrics&quot;) Correlate year variable from annual expenditures data to correct calendar year in order to adjust for inflation. rand_person_spend &lt;- mutate(rand_person_spend, expyear = indv_start_year + year - 1) Adjust spending for inflation. The CPI adjustment values below are based on the June CPI from 1991 (see table found at http://www.seattle.gov/financedepartment/cpi/historical.htm). cpi &lt;- tribble( ~ year, ~ cpi, 1973, 3.07, 1974, 2.76, 1975, 2.53, 1976, 2.39, 1977, 2.24, 1978, 2.09, 1979, 1.88, 1980, 1.65, 1981, 1.5, 1982, 1.41, 1983, 1.37, 1984, 1.31, 1985, 1.27 ) rand_person_spend &lt;- left_join(rand_person_spend, cpi, by = c(&quot;expyear&quot; = &quot;year&quot;)) %&gt;% mutate(out_inf = outsum * cpi, inpdol_inf = inpdol * cpi) Add a total spending variable. rand_person_spend &lt;- mutate(rand_person_spend, tot_inf = inpdol_inf + out_inf) Add a variable for any health insurance (free, Individual deductible, or cost-sharing): rand_person_spend &lt;- mutate(rand_person_spend, any_ins = plantype != &quot;Catastrophic&quot;) Count the number of observations in each plan-type, count(rand_person_spend, plantype) #&gt; # A tibble: 4 x 2 #&gt; plantype n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Catastrophic 3724 #&gt; 2 Deductible 4175 #&gt; 3 Cost Sharing 5464 #&gt; 4 Free 6840 and any-insurance, count(rand_person_spend, any_ins) #&gt; # A tibble: 2 x 2 #&gt; any_ins n #&gt; &lt;lgl&gt; &lt;int&gt; #&gt; 1 FALSE 3724 #&gt; 2 TRUE 16479 Create a list of response variables. varlist &lt;- c(&quot;ftf&quot;, &quot;out_inf&quot;, &quot;totadm&quot;, &quot;inpdol_inf&quot;, &quot;tot_inf&quot;) Calculate the mean and standard deviation for those receiving catastrophic insurance. rand_person_spend %&gt;% filter(plantype == &quot;Catastrophic&quot;) %&gt;% select(one_of(varlist)) %&gt;% gather(response, value) %&gt;% group_by(response) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), `Std. Dev.` = sd(value, na.rm = TRUE)) #&gt; # A tibble: 5 x 3 #&gt; response Mean `Std. Dev.` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ftf 2.78 5.50 #&gt; 2 inpdol_inf 388. 2308. #&gt; 3 out_inf 248. 488. #&gt; 4 tot_inf 636. 2535. #&gt; 5 totadm 0.0991 0.379 Calculate the difference in means between plans and the catastophic plan. calc_diffs &lt;- function(x) { # programmatically create the formula f &lt;- quo(!!sym(x) ~ plantype) mod &lt;- lm(f, data = rand_person_spend) # nolint out &lt;- cluster_se(mod, cluster = rand_person_spend[[&quot;fam_identifier&quot;]]) out[[&quot;response&quot;]] &lt;- x out } person_diffs &lt;- map_dfr(varlist, calc_diffs) %&gt;% select(response, term, estimate, std.error) %&gt;% mutate(term = str_replace(term, &quot;^plantype&quot;, &quot;&quot;)) Standard errors are clustered by family identifier using the clubSandwich package. Print the table. If this were an actual publication, I’d make it nicer. fmt_num &lt;- function(x) { prettyNum(x, digits = 3, format = &quot;f&quot;, big.mark = &quot;,&quot;, drop0trailing = FALSE) } person_diffs %&gt;% mutate(estimate = str_c(fmt_num(estimate), &quot; (&quot;, fmt_num(std.error), &quot;)&quot;)) %&gt;% select(-std.error) %&gt;% spread(term, estimate) %&gt;% knitr::kable(digits = 3) response (Intercept) Cost Sharing Deductible Free ftf 2.78 (0.178) 0.481 (0.24) 0.193 (0.247) 1.66 (0.248) inpdol_inf 388 (44.9) 92.5 (72.8) 72.2 (68.6) 116 (59.8) out_inf 248 (14.8) 59.8 (20.7) 41.8 (20.8) 169 (19.9) tot_inf 636 (54.5) 152 (84.6) 114 (79.1) 285 (72.4) totadm 0.0991 (0.00785) 0.0023 (0.0108) 0.0159 (0.0109) 0.0288 (0.0105) Additionally we could plot the difference-in-means of each plan type vs. catastrophic insurance. ggplot(filter(person_diffs, term != &quot;(Intercept)&quot;), aes(x = term, y = estimate, ymin = estimate - 2 * std.error, ymax = estimate + 2 * std.error)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 1) + geom_pointrange() + facet_grid(response ~ ., scales = &quot;free_y&quot;) References https://www.icpsr.umich.edu/icpsrweb/NACDA/studies/6439/version/1 http://masteringmetrics.com/wp-content/uploads/2015/01/ReadMe_RAND.txt http://masteringmetrics.com/wp-content/uploads/2015/01/Code.zip "],
["minneapolis-domestic-violence-experiment.html", "3 Minneapolis Domestic Violence Experiment 3.1 References", " 3 Minneapolis Domestic Violence Experiment This replicates Table 3.3 of Mastering ’Metrics, which replicates the Minneapolis Domestic Violence Experiment (Sherman and Berk 1984,@Angrist2006). Load necessary packages. library(&quot;tidyverse&quot;) Load the MDVE data. data(&quot;mdve&quot;, package = &quot;masteringmetrics&quot;) Randomized assignments (i.e. what are police assigned to do) are in the assigned column. Actual outcomes (i.e. what action do the police actually take) is in the outcome column. gen outcome = “Arrest” if T_FINAL == 1 replace outcome = “Advise” if T_FINAL == 2 replace outcome = “Separate” if T_FINAL == 3 replace outcome = “Other” if T_FINAL == 4 gen total = 1 mdve &lt;- mutate(mdve, assigned = case_when( T_RANDOM == 1 ~ &quot;Arrest&quot;, T_RANDOM == 2 ~ &quot;Advise&quot;, T_RANDOM == 3 ~ &quot;Separate&quot; ), outcome = case_when( T_FINAL == 1 ~ &quot;Arrest&quot;, T_FINAL == 2 ~ &quot;Advise&quot;, T_FINAL == 3 ~ &quot;Separate&quot;, T_FINAL == 4 ~ &quot;Other&quot; ), coddled_a = assigned != &quot;Arrest&quot;, coddled_o = outcome != &quot;Arrest&quot; ) %&gt;% filter(outcome != &quot;Other&quot;) Assigned and delivered treatments in the MDVE: mdve_summary &lt;- mdve %&gt;% count(assigned, outcome) %&gt;% group_by(assigned) %&gt;% mutate(p = n / sum(n)) print(mdve_summary, n = nrow(mdve_summary)) #&gt; # A tibble: 8 x 4 #&gt; # Groups: assigned [3] #&gt; assigned outcome n p #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Advise Advise 84 0.778 #&gt; 2 Advise Arrest 19 0.176 #&gt; 3 Advise Separate 5 0.0463 #&gt; 4 Arrest Arrest 91 0.989 #&gt; 5 Arrest Separate 1 0.0109 #&gt; 6 Separate Advise 5 0.0439 #&gt; 7 Separate Arrest 26 0.228 #&gt; 8 Separate Separate 83 0.728 Assigned proportions in the MDVE: mdve_assigned &lt;- mdve %&gt;% count(assigned) %&gt;% mutate(p = n / sum(n)) mdve_assigned #&gt; # A tibble: 3 x 3 #&gt; assigned n p #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Advise 108 0.344 #&gt; 2 Arrest 92 0.293 #&gt; 3 Separate 114 0.363 Delivered treatments in the MDVE: mdve_outcome &lt;- mdve %&gt;% count(outcome) %&gt;% mutate(p = n / sum(n)) mdve_outcome #&gt; # A tibble: 3 x 3 #&gt; outcome n p #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Advise 89 0.283 #&gt; 2 Arrest 136 0.433 #&gt; 3 Separate 89 0.283 Probability of being coddled, given being assigned the coddled treatment: mdve_coddled &lt;- mdve %&gt;% count(coddled_a, coddled_o) %&gt;% group_by(coddled_a) %&gt;% mutate(p = n / sum(n)) mdve_coddled #&gt; # A tibble: 4 x 4 #&gt; # Groups: coddled_a [2] #&gt; coddled_a coddled_o n p #&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 FALSE FALSE 91 0.989 #&gt; 2 FALSE TRUE 1 0.0109 #&gt; 3 TRUE FALSE 45 0.203 #&gt; 4 TRUE TRUE 177 0.797 IV first stage, \\[ E[D_i | Z_i = 1] - E[D_i | Z_i = 0] . \\] filter(mdve_coddled, coddled_o, coddled_a)$p - filter(mdve_coddled, coddled_o, !coddled_a)$p #&gt; [1] 0.786 The response variable is not provided, so the full 2SLS is not estimated here. 3.1 References http://masteringmetrics.com/wp-content/uploads/2015/02/MDVE_Table33.do http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_MDVE.txt "],
["mlda-regression-discontinuity.html", "4 MLDA Regression Discontinuity 4.1 References", " 4 MLDA Regression Discontinuity MLDA Regression Discontinuity (based on data from Carpenter and Dobkin (2011)) from Chapter 4 of Mastering ’Metrics, Table 4.1 and Figures 4.2, 4.4, and 4.5 in Mastering Metrics. These present sharp RD estimates of the effect of the minimum legal drinking age (MLDA) on mortality. Load libraries. library(&quot;tidyverse&quot;) library(&quot;haven&quot;) library(&quot;rlang&quot;) library(&quot;broom&quot;) library(&quot;lmtest&quot;) library(&quot;sandwich&quot;) Load MLDA data data(&quot;mlda&quot;, package = &quot;masteringmetrics&quot;) Add an indicator variable for individuals over 21 years of age. mlda &lt;- mutate(mlda, age = agecell - 21, over21 = as.integer(agecell &gt;= 21)) Add a variable for other causes of death. mlda &lt;- mutate(mlda, ext_oth = external - homicide - suicide - mva) For “all causes”, “motor vehicle accidents”, and “internal causes” deaths plot the linear and quadratic trends on each side of age 21. varlist &lt;- c(&quot;all&quot; = &quot;All Causes&quot;, &quot;mva&quot; = &quot;Motor Vehicle Accidents&quot;, &quot;internal&quot; = &quot;Internal Causes&quot;) mlda %&gt;% select(agecell, over21, one_of(names(varlist))) %&gt;% gather(response, value, -agecell, -over21, na.rm = TRUE) %&gt;% mutate(response = recode(response, !!!as.list(varlist))) %&gt;% ggplot(aes(x = agecell, y = value)) + geom_point() + geom_smooth(mapping = aes(group = over21), se = FALSE, method = &quot;lm&quot;, formula = y ~ poly(x, 2)) + geom_smooth(mapping = aes(group = over21), se = FALSE, method = &quot;lm&quot;, formula = y ~ x, color = &quot;black&quot;) + facet_grid(response ~ ., scales = &quot;free_y&quot;) + labs(y = &quot;Mortality rate (per 100,000)&quot;, x = &quot;Age&quot;) responses &lt;- c(&quot;all&quot; = &quot;All deaths&quot;, &quot;mva&quot; = &quot;Motor vehicle accidents&quot;, &quot;suicide&quot; = &quot;Suicide&quot;, &quot;homicide&quot; = &quot;Homocide&quot;, &quot;ext_oth&quot; = &quot;Other external causes&quot;, &quot;internal&quot; = &quot;All internal causes&quot;, &quot;alcohol&quot; = &quot;Alcohol&quot;) Define a function to run four regressions for a given response variable, y. run_reg &lt;- function(y) { mods &lt;- list( &quot;Ages 19-22, Linear&quot; = lm(quo(!!sym(y) ~ age * over21), data = mlda), &quot;Ages 19-22, Quadratic&quot; = lm(quo(!!sym(y) ~ poly(age, 2, raw = TRUE) * over21), data = mlda), &quot;Ages 20-21, Linear&quot; = lm(quo(!!sym(y) ~ age * over21), data = filter(mlda, agecell &gt;= 20, agecell &lt;= 22)), &quot;Ages 20-21, Quadratic&quot; = lm(quo(!!sym(y) ~ poly(age, 2, raw = TRUE) * over21), data = filter(mlda, agecell &gt;= 20, agecell &lt;= 22)) ) out &lt;- tibble( model_name = names(mods), model = mods, ages = rep(c(&quot;19-22&quot;, &quot;20-21&quot;), each = 2), trend = rep(c(&quot;Linear&quot;, &quot;Quadratic&quot;), 2), model_num = seq_along(mods) ) %&gt;% mutate(coefs = map(model, ~ tidy(coeftest(.x, vcovHC(.x))))) %&gt;% # nolint unnest(coefs, .drop = FALSE) %&gt;% filter(term == &quot;over21&quot;) %&gt;% select(model_name, model, term, estimate, std.error) %&gt;% mutate(response = y) # sample size = df.residuals + residuals out[[&quot;obs&quot;]] &lt;- map_dfr(mods, glance) %&gt;% mutate(obs = df.residual + df) %&gt;% pluck(&quot;obs&quot;) out } mlda_regs &lt;- map_dfr(names(responses), run_reg) %&gt;% mutate(response = recode(response, !!!as.list(responses))) mlda_regs %&gt;% select(model_name, response, estimate, std.error) %&gt;% gather(stat, value, estimate, std.error) %&gt;% spread(model_name, value) %&gt;% knitr::kable() response stat Ages 19-22, Linear Ages 19-22, Quadratic Ages 20-21, Linear Ages 20-21, Quadratic Alcohol estimate 0.442 0.799 0.740 1.028 Alcohol std.error 0.213 0.431 0.360 0.725 All deaths estimate 7.663 9.548 9.753 9.611 All deaths std.error 1.374 2.231 2.279 3.565 All internal causes estimate 0.392 1.073 1.692 1.249 All internal causes std.error 0.592 0.931 0.877 1.465 Homocide estimate 0.104 0.200 0.164 -0.453 Homocide std.error 0.394 0.604 0.590 1.594 Motor vehicle accidents estimate 4.534 4.663 4.759 5.892 Motor vehicle accidents std.error 0.731 1.366 1.385 1.937 Other external causes estimate 0.838 1.797 1.414 1.625 Other external causes std.error 0.413 0.673 0.606 1.245 Suicide estimate 1.794 1.814 1.724 1.297 Suicide std.error 0.530 0.950 0.881 1.661 The robust standard errors using the H C3 standard errors fr om sandwich::vcovHC and dif fer from tho se reported in *Maste ring ’Metrics*. 4.1 References http://masteringmetrics.com/wp-content/uploads/2015/01/master_cd_rd.do http://masteringmetrics.com/wp-content/uploads/2015/01/ReadMe_MLDA.txt "],
["mississippi-bank-failures-in-the-great-depression.html", "5 Mississippi Bank Failures in the Great Depression 5.1 References", " 5 Mississippi Bank Failures in the Great Depression A difference-in-difference analysis of Mississippi bank failures during the Great Depression (Richardson and Troost 2009). This replicates Figures 5.1–5.3 in Mastering ’Metrics. library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) Load the banks data. data(&quot;banks&quot;, package = &quot;masteringmetrics&quot;) Only use yearly data in the difference-in-difference estimates. Use the number of banks on July 1st of each year. banks &lt;- banks %&gt;% filter(month(date) == 7L, mday(date) == 1L) %&gt;% mutate(year = year(date)) %&gt;% select(year, matches(&quot;bi[ob][68]&quot;)) Generate the counterfactual using the difference between the number of banks in district 8 and district 6. banks &lt;- banks %&gt;% arrange(year) %&gt;% mutate(diff86 = bib8[year == 1930] - bib6[year == 1930], counterfactual = if_else(year &gt;= 1930, bib8 - diff86, NA_integer_)) %&gt;% select(-diff86) Plot the lines of the Distinct 8 banks in business, District 6 banks in business, and the District 6 counterfactual. This is equivalent to Figure 5.3 of Angrist and Pischke (2014). select(banks, year, bib8, bib6, counterfactual) %&gt;% gather(variable, value, -year, na.rm = TRUE) %&gt;% mutate(variable = recode(variable, bib8 = &quot;8th district&quot;, bib6 = &quot;6th district&quot;, counterfactual = &quot;Counterfactual&quot;)) %&gt;% ggplot(aes(x = year, y = value, colour = variable)) + geom_point() + geom_line() + ylab(&quot;Number of Banks in Business&quot;) + xlab(&quot;&quot;) Plot the difference-in-difference estimate for all years after 1930. ggplot(filter(banks, year &gt; 1930), aes(x = year, y = bib6 - counterfactual)) + geom_point() + geom_line() + ylab(&quot;DID (Number of Banks)&quot;) + xlab(&quot;&quot;) Figure 5.1: Difference between Eighth District and Sixth District Counterfactuals 5.1 References http://masteringmetrics.com/wp-content/uploads/2015/02/master_banks.do http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_BankFailures.txt "],
["mlda-difference-in-difference.html", "6 MLDA Difference-in-Difference 6.1 Table 5.2 6.2 Table 5.3 6.3 References", " 6 MLDA Difference-in-Difference Difference-in-difference estimates of the effect of the minimum legal drinking age (MLDA) on mortality (Mouchel, Williams, and Zador 1987; Norberg, Bierut, and Grucza 2009). This replicates the analyses in Tables 5.2 and 5.3 in Mastering ’Metrics. Load necessary libraries. library(&quot;tidyverse&quot;) library(&quot;haven&quot;) library(&quot;rlang&quot;) library(&quot;broom&quot;) library(&quot;clubSandwich&quot;) data(&quot;deaths&quot;, package = &quot;masteringmetrics&quot;) In these regressions, we will use both indicator variables for year as well as a trend, so make a factor version of the year variable. deaths &lt;- mutate(deaths, year_fct = factor(year)) 6.1 Table 5.2 Regression DD Estimates of MLDA-Induced Deaths among 18-20 year-olds, from 1970-1983 dtypes &lt;- c(&quot;all&quot; = &quot;All deaths&quot;, &quot;MVA&quot; = &quot;Motor vehicle accidents&quot;, &quot;suicide&quot; = &quot;Suicide&quot;, &quot;internal&quot; = &quot;All internal causes&quot;) Estimate the DD for MLDA for all causes of death in 18-20 year olds. Run the regression with lm and calculate the cluster robust standard errors using sandwich::vcovCL. Subset the data. data &lt;- filter(deaths, year &lt;= 1983, agegr == &quot;18-20 yrs&quot;, dtype == &quot;all&quot;) Run the OLS model. mod &lt;- lm(mrate ~ 0 + legal + state + year_fct, data = data) Calculate cluster robust coefficients. These are calculated using a different method than Stata uses, and thus will be slightly different than those reported in the book. vcov &lt;- vcovCR(mod, cluster = data[[&quot;state&quot;]], type = &quot;CR2&quot;) coef_test(mod, vcov = vcov) %&gt;% rownames_to_column(var = &quot;term&quot;) %&gt;% as_tibble() %&gt;% select(term, estimate = beta, std.error = SE) %&gt;% filter(term == &quot;legal&quot;) %&gt;% knitr::kable(digits = 2) term estimate std.error legal 10.8 4.48 Function to calculate clustered standard errors and return a tidy data frame of the coefficients and standard errors. cluster_se &lt;- function(mod, cluster, type = &quot;CR2&quot;) { vcov &lt;- vcovCR(mod, cluster = cluster, type = &quot;CR2&quot;) coef_test(mod, vcov = vcov) %&gt;% rownames_to_column(var = &quot;term&quot;) %&gt;% as_tibble() %&gt;% select(term, estimate = beta, std.error = SE) } run_mlda_dd &lt;- function(i) { data &lt;- filter(deaths, year &lt;= 1983, agegr == &quot;18-20 yrs&quot;, dtype == i) # nolint mods &lt;- tribble( ~ name, ~ model, &quot;No trends, no weights&quot;, lm(mrate ~ 0 + legal + state + year_fct, data = data), &quot;Time trends, no weights&quot;, lm(mrate ~ 0 + legal + year_fct + state + state:year, data = data), &quot;No trends, weights&quot;, lm(mrate ~ 0 + legal + year_fct + state, data = data, weights = pop), # nolint start # &quot;Time trends, weights&quot;, # lm(mrate ~ 0 + legal + year_fct + state + state:year, # data = data, weights = pop) # nolint end ) %&gt;% mutate(coefs = map(model, ~ cluster_se(.x, cluster = data[[&quot;state&quot;]], type = &quot;CR2&quot;))) %&gt;% unnest(coefs) %&gt;% filter(term == &quot;legal&quot;) %&gt;% mutate(response = i) %&gt;% select(name, response, estimate, std.error) } mlda_dd &lt;- map_df(names(dtypes), run_mlda_dd) mlda_dd %&gt;% knitr::kable(digits = 2) name response estimate std.error No trends, no weights all 10.80 4.48 Time trends, no weights all 8.47 4.74 No trends, weights all 12.41 4.78 No trends, no weights MVA 7.59 2.43 Time trends, no weights MVA 6.64 2.47 No trends, weights MVA 7.50 2.30 No trends, no weights suicide 0.59 0.57 Time trends, no weights suicide 0.47 0.74 No trends, weights suicide 1.49 0.92 No trends, no weights internal 1.33 1.53 Time trends, no weights internal 0.08 1.80 No trends, weights internal 1.89 1.83 6.2 Table 5.3 Regression DD Estimates of MLDA-Induced Deaths among 18-20 year-olds, from 1970-1983, controlling for Beer Taxes. This is the analysis presented in Angrist and Pischke (2014) Table 5.3. run_beertax &lt;- function(i) { data &lt;- filter(deaths, year &lt;= 1983, agegr == &quot;18-20 yrs&quot;, dtype == i, !is.na(beertaxa)) out &lt;- tribble( ~ name, ~ model, &quot;No time trends&quot;, lm(mrate ~ 0 + legal + beertaxa + year_fct + state, data = data), &quot;Time trends&quot;, lm(mrate ~ 0 + legal + beertaxa + year_fct + state + state:year, data = data) ) %&gt;% # calc culstered standard errors mutate(coefs = map(model, ~ cluster_se(.x, data[[&quot;state&quot;]]))) %&gt;% unnest(coefs) %&gt;% filter(term %in% c(&quot;legal&quot;, &quot;beertaxa&quot;)) %&gt;% mutate(response = i) %&gt;% select(response, name, term, estimate, std.error) } beertax &lt;- map_df(names(dtypes), run_beertax) beertax %&gt;% knitr::kable(digits = 2) response name term estimate std.error all No time trends legal 10.98 4.60 all No time trends beertaxa 1.51 9.02 all Time trends legal 10.03 4.57 all Time trends beertaxa -5.52 30.40 MVA No time trends legal 7.59 2.51 MVA No time trends beertaxa 3.82 5.27 MVA Time trends legal 6.89 2.47 MVA Time trends beertaxa 26.88 18.76 suicide No time trends legal 0.45 0.58 suicide No time trends beertaxa -3.05 1.61 suicide Time trends legal 0.38 0.72 suicide Time trends beertaxa -12.13 8.28 internal No time trends legal 1.46 1.56 internal No time trends beertaxa -1.36 3.02 internal Time trends legal 0.88 1.68 internal Time trends beertaxa -10.31 10.90 Note: I had trouble getting sandwich::vcovCL to estimate clustered standard errors for this regression. 6.3 References http://masteringmetrics.com/wp-content/uploads/2015/01/analysis.do http://masteringmetrics.com/wp-content/uploads/2015/01/ReadMe_MLDA_DD.txt "],
["twins-and-returns-to-schooling.html", "7 Twins and Returns to Schooling References", " 7 Twins and Returns to Schooling Estimates of the returns to schooling for Twinsburg twins (Ashenfelter and Krueger 1994; Ashenfelter and Rouse 1998). This replicates the analysis in Table 6.2 of Mastering ’Metrics. library(&quot;tidyverse&quot;) library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) library(&quot;AER&quot;) Load twins data. data(&quot;pubtwins&quot;, package = &quot;masteringmetrics&quot;) Run a regression of log wage on controls (Column 1 of Table 6.2). mod1 &lt;- lm(lwage ~ educ + poly(age, 2) + female + white, data = pubtwins) coeftest(mod1, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.1791 0.1631 7.23 1.3e-12 *** #&gt; educ 0.1100 0.0104 10.54 &lt; 2e-16 *** #&gt; poly(age, 2)1 4.9643 0.5697 8.71 &lt; 2e-16 *** #&gt; poly(age, 2)2 -4.2957 0.5919 -7.26 1.1e-12 *** #&gt; female -0.3180 0.0397 -8.00 5.4e-15 *** #&gt; white -0.1001 0.0679 -1.47 0.14 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note: The age coefficients are different (but equivalent) to those reported in the Table due to the the use of poly(age, .), which calculates orthogonal polynomials. Run regression of the difference in log wage between twins on the difference in education (Column 2 of Table 6.2). mod2 &lt;- lm(dlwage ~ deduc, data = filter(pubtwins, first == 1)) coeftest(mod2, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.0296 0.0275 1.07 0.2835 #&gt; deduc 0.0610 0.0198 3.09 0.0022 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Run a regression of log wage on controls, instrumenting education with twin’s education (Column 3 of Table 6.2). mod3 &lt;- ivreg(lwage ~ educ + poly(age, 2) + female + white | . - educ + educt, data = pubtwins) summary(mod3, vcov = sandwich, diagnostics = TRUE) #&gt; #&gt; Call: #&gt; ivreg(formula = lwage ~ educ + poly(age, 2) + female + white | #&gt; . - educ + educt, data = pubtwins) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.69585 -0.29218 0.00494 0.26262 2.47060 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.0636 0.2113 5.03 6.2e-07 *** #&gt; educ 0.1179 0.0137 8.62 &lt; 2e-16 *** #&gt; poly(age, 2)1 5.0367 0.5805 8.68 &lt; 2e-16 *** #&gt; poly(age, 2)2 -4.2897 0.5928 -7.24 1.3e-12 *** #&gt; female -0.3149 0.0403 -7.81 2.2e-14 *** #&gt; white -0.0974 0.0682 -1.43 0.15 #&gt; #&gt; Diagnostic tests: #&gt; df1 df2 statistic p-value #&gt; Weak instruments 1 674 796.30 &lt;2e-16 *** #&gt; Wu-Hausman 1 673 0.92 0.34 #&gt; Sargan 0 NA NA NA #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.507 on 674 degrees of freedom #&gt; Multiple R-Squared: 0.338, Adjusted R-squared: 0.333 #&gt; Wald test: 56.8 on 5 and 674 DF, p-value: &lt;2e-16 Note: The coefficient for years of education is slightly different than that reported in the book. Run a regression of the difference in wage, instrumenting the difference in years of education with twin’s education (Column 4 of Table 6.2). mod4 &lt;- ivreg(dlwage ~ deduc | deduct, data = filter(pubtwins, first == 1)) summary(mod4, vcov = sandwich, diagnostics = TRUE) #&gt; #&gt; Call: #&gt; ivreg(formula = dlwage ~ deduc | deduct, data = filter(pubtwins, #&gt; first == 1)) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.0423 -0.3111 -0.0274 0.2471 2.0824 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.0274 0.0277 0.99 0.3237 #&gt; deduc 0.1070 0.0339 3.15 0.0018 ** #&gt; #&gt; Diagnostic tests: #&gt; df1 df2 statistic p-value #&gt; Weak instruments 1 338 85.15 &lt;2e-16 *** #&gt; Wu-Hausman 1 337 4.12 0.043 * #&gt; Sargan 0 NA NA NA #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.512 on 338 degrees of freedom #&gt; Multiple R-Squared: 0.0132, Adjusted R-squared: 0.0103 #&gt; Wald test: 9.94 on 1 and 338 DF, p-value: 0.00176 Note: The coefficient for years of education is slightly different than that reported in the book. References http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_Twinsburg.txt http://masteringmetrics.com/wp-content/uploads/2015/02/twins.do "],
["child-labor-laws-as-an-iv.html", "8 Child Labor Laws as an IV 8.1 First stages and reduced forms 8.2 IV returns References", " 8 Child Labor Laws as an IV 2SLS estimates of the returns to schooling using child labor laws as instruments for years of schooling (Acemoglu and Angrist 2000). This replicates Table 6.3 of Mastering ’Metrics. library(&quot;AER&quot;) library(&quot;sandwich&quot;) library(&quot;clubSandwich&quot;) library(&quot;tidyverse&quot;) library(&quot;broom&quot;) Load the child_labor data. data(&quot;child_labor&quot;, package = &quot;masteringmetrics&quot;) child_labor &lt;- mutate(child_labor, year = factor(year), yob_fct = factor(yob), sob = factor(sob)) 8.1 First stages and reduced forms Column 1. Years of Schooling. mod1 &lt;- lm(indEduc ~ year + yob_fct + sob + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod1, vcov = vcovCR(mod1, cluster = child_labor[[&quot;sob&quot;]])) Column 2. Years of Schooling. State of birth dummies x linear year of birth trends. mod2 &lt;- lm(indEduc ~ year + yob_fct + sob + sob:yob + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod2, vcov = vcovCR(mod2, cluster = child_labor[[&quot;sob&quot;]])) Column 3. Log weekly wages. mod3 &lt;- lm(lnwkwage ~ year + yob_fct + sob + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod3, vcov = vcovCR(mod1), cluster = child_labor[[&quot;state&quot;]]) Column 4. Log weekly wages. State of birth dummies x linear year of birth trends. mod4 &lt;- lm(lnwkwage ~ year + yob_fct + sob + sob:yob + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod4, vcov = vcovCR(mod2), cluster = child_labor[[&quot;state&quot;]]) 8.2 IV returns Column 3. Log weekly wages. mod5 &lt;- ivreg(lnwkwage ~ year + yob_fct + sob + indEduc | . - indEduc + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod5, vcov = vcovCR(mod1), cluster = child_labor[[&quot;state&quot;]]) Column 4. Log weekly wages. State of birth dummies x linear year of birth trends. mod6 &lt;- ivreg(lnwkwage ~ year + yob_fct + sob + sob:yob + indEduc | . - indEduc + cl7 + cl8 + cl9, data = child_labor, weights = weight) # coef_test(mod6, vcov = vcovCR(mod2), cluster = child_labor[[&quot;state&quot;]]) References http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_ChildLaborLaws.txt http://masteringmetrics.com/wp-content/uploads/2015/02/AA_regs.do "],
["quarter-of-birth-and-returns-to-schooling.html", "9 Quarter of Birth and Returns to Schooling 9.1 Table 6.5 9.2 Figures References", " 9 Quarter of Birth and Returns to Schooling This replicates Tables 6.4 and 6.5, and Figures 6.1 and 6.2 of Mastering ’Metrics. These present an IV analysis of the returns to schooling using quarters of birth (QOB) as instruments for years of schooling (Angrist and Krueger 1991). library(&quot;AER&quot;) library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) library(&quot;tidyverse&quot;) library(&quot;broom&quot;) Load twins data. data(&quot;ak91&quot;, package = &quot;masteringmetrics&quot;) Some cleaning of the data. ak91 &lt;- mutate(ak91, qob_fct = factor(qob), q4 = as.integer(qob == &quot;4&quot;), yob_fct = factor(yob)) Table 6.4. IV recipe for returns to schooling using a single QOB instrument. Regress log wages on 4th quarter. mod1 &lt;- lm(lnw ~ q4, data = ak91) coeftest(mod1, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 5.89827 0.00136 4329.13 &lt;2e-16 *** #&gt; q4 0.00681 0.00274 2.48 0.013 * #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Regress years of schooling on 4th quarter. mod2 &lt;- lm(s ~ q4, data = ak91) coeftest(mod2, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 12.74731 0.00661 1929 &lt; 2e-16 *** #&gt; q4 0.09212 0.01316 7 2.6e-12 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 IV regression of log wages on years of schooling, with 4th quarter as an instrument for years of schooling. mod3 &lt;- ivreg(lnw ~ s | q4, data = ak91) coeftest(mod3, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.955 0.358 13.85 &lt;2e-16 *** #&gt; s 0.074 0.028 2.64 0.0083 ** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.1 Table 6.5 Regression Estimates of Returns to Schooling using Quarter of Birth Instruments Column 1. OLS mod4 &lt;- lm(lnw ~ s, data = ak91) coeftest(mod4, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.995182 0.005074 984 &lt;2e-16 *** #&gt; s 0.070851 0.000381 186 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Column 2. IV with only the 4th quarter as an instrument. mod5 &lt;- ivreg(lnw ~ s | q4, data = ak91) summary(mod5, vcov = sandwich, diagnostics = TRUE) #&gt; #&gt; Call: #&gt; ivreg(formula = lnw ~ s | q4, data = ak91) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.7765 -0.2393 0.0713 0.3326 4.6536 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.955 0.358 13.85 &lt;2e-16 *** #&gt; s 0.074 0.028 2.64 0.0083 ** #&gt; #&gt; Diagnostic tests: #&gt; df1 df2 statistic p-value #&gt; Weak instruments 1 329507 48.99 2.6e-12 *** #&gt; Wu-Hausman 1 329506 0.01 0.91 #&gt; Sargan 0 NA NA NA #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.638 on 329507 degrees of freedom #&gt; Multiple R-Squared: 0.117, Adjusted R-squared: 0.117 #&gt; Wald test: 6.97 on 1 and 329507 DF, p-value: 0.00829 The argument diagnostics = TRUE will run an F-test on the first stage which is reported as the “Weak instruments” diagnostic. Column 3. OLS. Controls for year of birth. mod6 &lt;- lm(lnw ~ s + yob_fct, data = ak91) coeftest(mod6, vcov = sandwich) #&gt; #&gt; t test of coefficients: #&gt; #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 5.017348 0.006019 833.65 &lt; 2e-16 *** #&gt; s 0.071081 0.000381 186.34 &lt; 2e-16 *** #&gt; yob_fct31 -0.006387 0.005123 -1.25 0.21251 #&gt; yob_fct32 -0.014838 0.005052 -2.94 0.00331 ** #&gt; yob_fct33 -0.017583 0.005068 -3.47 0.00052 *** #&gt; yob_fct34 -0.020999 0.005062 -4.15 3.3e-05 *** #&gt; yob_fct35 -0.032895 0.005039 -6.53 6.7e-11 *** #&gt; yob_fct36 -0.031781 0.004970 -6.39 1.6e-10 *** #&gt; yob_fct37 -0.036712 0.004894 -7.50 6.4e-14 *** #&gt; yob_fct38 -0.036890 0.004856 -7.60 3.1e-14 *** #&gt; yob_fct39 -0.048164 0.004833 -9.96 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Column 4. IV reg using only the 4th quarter as an instrument. Controls for year of birth. mod7 &lt;- ivreg(lnw ~ s + yob_fct | q4 + yob_fct, data = ak91) summary(mod7, vcov = sandwich, diagnostics = TRUE) #&gt; #&gt; Call: #&gt; ivreg(formula = lnw ~ s + yob_fct | q4 + yob_fct, data = ak91) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.7785 -0.2346 0.0719 0.3405 4.6687 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.96599 0.35393 14.03 &lt;2e-16 *** #&gt; s 0.07520 0.02841 2.65 0.0081 ** #&gt; yob_fct31 -0.00696 0.00647 -1.08 0.2819 #&gt; yob_fct32 -0.01557 0.00708 -2.20 0.0279 * #&gt; yob_fct33 -0.01855 0.00833 -2.23 0.0259 * #&gt; yob_fct34 -0.02209 0.00909 -2.43 0.0151 * #&gt; yob_fct35 -0.03425 0.01061 -3.23 0.0012 ** #&gt; yob_fct36 -0.03338 0.01208 -2.76 0.0057 ** #&gt; yob_fct37 -0.03857 0.01368 -2.82 0.0048 ** #&gt; yob_fct38 -0.03910 0.01596 -2.45 0.0143 * #&gt; yob_fct39 -0.05053 0.01705 -2.96 0.0030 ** #&gt; #&gt; Diagnostic tests: #&gt; df1 df2 statistic p-value #&gt; Weak instruments 1 329498 47.73 4.9e-12 *** #&gt; Wu-Hausman 1 329497 0.02 0.88 #&gt; Sargan 0 NA NA NA #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.638 on 329498 degrees of freedom #&gt; Multiple R-Squared: 0.117, Adjusted R-squared: 0.117 #&gt; Wald test: 1.81 on 10 and 329498 DF, p-value: 0.054 Column 4. IV reg using all quarters as instruments. Controls for year of birth. mod8 &lt;- ivreg(lnw ~ s + yob_fct | qob_fct + yob_fct, data = ak91) summary(mod8, vcov = sandwich, diagnostics = TRUE) #&gt; #&gt; Call: #&gt; ivreg(formula = lnw ~ s + yob_fct | qob_fct + yob_fct, data = ak91) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -8.9945 -0.2544 0.0676 0.3509 4.8425 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 4.59174 0.25057 18.32 &lt; 2e-16 *** #&gt; s 0.10525 0.02012 5.23 1.7e-07 *** #&gt; yob_fct31 -0.01111 0.00591 -1.88 0.05988 . #&gt; yob_fct32 -0.02089 0.00623 -3.35 0.00080 *** #&gt; yob_fct33 -0.02556 0.00698 -3.66 0.00025 *** #&gt; yob_fct34 -0.03007 0.00742 -4.05 5.1e-05 *** #&gt; yob_fct35 -0.04414 0.00836 -5.28 1.3e-07 *** #&gt; yob_fct36 -0.04501 0.00930 -4.84 1.3e-06 *** #&gt; yob_fct37 -0.05207 0.01034 -5.04 4.7e-07 *** #&gt; yob_fct38 -0.05518 0.01184 -4.66 3.1e-06 *** #&gt; yob_fct39 -0.06780 0.01259 -5.39 7.2e-08 *** #&gt; #&gt; Diagnostic tests: #&gt; df1 df2 statistic p-value #&gt; Weak instruments 3 329496 32.32 &lt;2e-16 *** #&gt; Wu-Hausman 1 329497 2.98 0.084 . #&gt; Sargan 2 NA 3.26 0.196 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.647 on 329498 degrees of freedom #&gt; Multiple R-Squared: 0.0905, Adjusted R-squared: 0.0905 #&gt; Wald test: 3.79 on 10 and 329498 DF, p-value: 3.9e-05 9.2 Figures Summarize the average wages by age: ak91_age &lt;- ak91 %&gt;% group_by(qob, yob) %&gt;% summarise(lnw = mean(lnw), s = mean(s)) %&gt;% mutate(q4 = (qob == 4)) Average years of schooling by quarter of birth for men born in 1930-39 in the 1980 US Census. ggplot(ak91_age, aes(x = yob + (qob - 1) / 4, y = s)) + geom_line() + geom_label(mapping = aes(label = qob, color = q4)) + theme(legend.position = &quot;none&quot;) + scale_x_continuous(&quot;Year of birth&quot;, breaks = 1930:1940) + scale_y_continuous(&quot;Years of Education&quot;, breaks = seq(12.2, 13.2, by = 0.2), limits = c(12.2, 13.2)) Average log wages by quarter of birth for men born in 1930-39 in the 1980 US Census. ggplot(ak91_age, aes(x = yob + (qob - 1) / 4, y = lnw)) + geom_line() + geom_label(mapping = aes(label = qob, color = q4)) + scale_x_continuous(&quot;Year of birth&quot;, breaks = 1930:1940) + scale_y_continuous(&quot;Log weekly wages&quot;) + theme(legend.position = &quot;none&quot;) References http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_QOB.txt http://masteringmetrics.com/wp-content/uploads/2015/02/ak91.do "],
["sheepskin-and-returns-to-schooling.html", "10 Sheepskin and Returns to Schooling 10.1 Figure 1 10.2 Figure 2 References", " 10 Sheepskin and Returns to Schooling This replicates Figures 6.3 and 6.4 of Mastering ’Metrics. These analyses use a fuzzy RD design to analyze the “sheepskin effects” of a high school diploma (Clark and Martorell 2014). library(&quot;tidyverse&quot;) Load sheepskin data. data(&quot;sheepskin&quot;, package = &quot;masteringmetrics&quot;) Create indicator variable for passing the test. sheepskin &lt;- mutate(sheepskin, test_lcs_pass = (minscore &gt;= 0)) 10.1 Figure 1 Figure 1. Regression discontinuity mod1_lhs &lt;- lm(receivehsd ~ poly(minscore, 4), data = filter(sheepskin, minscore &lt; 0), weights = n) mod1_rhs &lt;- lm(receivehsd ~ poly(minscore, 4), data = filter(sheepskin, minscore &gt;= 0), weights = n) Append fitted values to the original dataset fig1_data &lt;- sheepskin %&gt;% select(minscore, receivehsd, n) %&gt;% modelr::add_predictions(mod1_lhs, var = &quot;fit_hsd2_l&quot;) %&gt;% mutate(fit_hsd2_l = if_else(minscore &gt; 0, NA_real_, fit_hsd2_l)) %&gt;% modelr::add_predictions(mod1_rhs, var = &quot;fit_hsd2_r&quot;) %&gt;% mutate(fit_hsd2_r = if_else(minscore &lt; 0, NA_real_, fit_hsd2_r)) Figure 6.3. ggplot(fig1_data, aes(x = minscore)) + geom_vline(xintercept = 0, color = &quot;white&quot;, size = 2) + geom_point(mapping = aes(y = receivehsd), color = &quot;gray&quot;) + geom_line(mapping = aes(y = fit_hsd2_l)) + geom_line(mapping = aes(y = fit_hsd2_r)) + scale_x_continuous(&quot;Test Scores Relative to Cutoff&quot;, breaks = seq(-30, 15, by = 5), limits = c(-30, 15)) + scale_y_continuous(&quot;Fraction Receiving Diplomas&quot;, breaks = seq(0, 1, by = 0.2), limits = c(0, 1)) (#fig:fig.6.3)Last-chance exams and Texas sheepskin 10.2 Figure 2 mod2_lhs &lt;- lm(avgearnings ~ poly(minscore, 4), data = filter(sheepskin, minscore &lt; 0), weights = n) mod2_rhs &lt;- lm(avgearnings ~ poly(minscore, 4), data = filter(sheepskin, minscore &gt;= 0), weights = n) Append fitted values to the original dataset fig2_data &lt;- sheepskin %&gt;% select(minscore, avgearnings, n) %&gt;% modelr::add_predictions(mod2_lhs, var = &quot;fit_l&quot;) %&gt;% mutate(fit_l = if_else(minscore &gt; 0, NA_real_, fit_l)) %&gt;% modelr::add_predictions(mod2_rhs, var = &quot;fit_r&quot;) %&gt;% mutate(fit_r = if_else(minscore &lt; 0, NA_real_, fit_r)) Figure 6.4. ggplot(fig2_data, aes(x = minscore)) + geom_vline(xintercept = 0, color = &quot;white&quot;, size = 2) + geom_point(mapping = aes(y = avgearnings), color = &quot;gray&quot;) + geom_line(mapping = aes(y = fit_l)) + geom_line(mapping = aes(y = fit_r)) + scale_x_continuous(&quot;Test Scores Relative to Cutoff&quot;, breaks = seq(-30, 15, by = 5), limits = c(-30, 15)) + scale_y_continuous(&quot;Annual Earnings&quot;, breaks = seq(8000, 18000, by = 2000), limits = c(8000, 18000), labels = scales::comma_format()) (#fig:fig.6.4)The effect of last-chance exam scores on earnings References http://masteringmetrics.com/wp-content/uploads/2015/02/ReadMe_Sheepskin.txt http://masteringmetrics.com/wp-content/uploads/2015/02/cm_graphs.do "],
["references-10.html", "References", " References "]
]
